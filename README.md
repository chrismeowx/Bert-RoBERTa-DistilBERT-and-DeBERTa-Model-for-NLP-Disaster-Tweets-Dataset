# Comparison of 4 Models based on BERT ðŸ¤–

This is done to investigate which BERT models can give the best result using the same data preprocessing and training process. Four BERT models will be used are RoBERTa-base, BERT, DeBERTa, and DistilBERT. Based on accuracy,  RoBERTa manages to give the best accuracy result. Followed by DeBERTa as the second model which produces 0.82 score on accuracy.
Last places are occupied by BERT and DistilBERT with the same score which is 0.78. The complete code can be access on main.py file.

The Disaster Tweet Dataset can be accessed here: https://www.kaggle.com/competitions/nlp-getting-started 

